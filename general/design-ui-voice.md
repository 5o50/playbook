# Voice **User Interface**

[https://karpathy.github.io/2019/04/25/recipe/](https://karpathy.github.io/2019/04/25/recipe/)

[https://www.nature.com/articles/s41586-019-1119-1](https://www.nature.com/articles/s41586-019-1119-1)

[https://github.com/cloudkj/layer](https://github.com/cloudkj/layer)

[https://www.eetimes.com/author.asp?section_id=36&doc_id=1334594#](https://www.eetimes.com/author.asp?section_id=36&doc_id=1334594#)

> Online Streaming End-To-End Speech Recognition LSTM RNN-DNN

> C++ infra: —> voice stream to VaD to DS to inference to output

> Andrew Ng on Youtube — [https://www.youtube.com/watch?v=OXFZYChtYko](https://www.youtube.com/watch?v=OXFZYChtYko)

[https://news.ycombinator.com/item?id=19712465](https://news.ycombinator.com/item?id=19712465)

[https://en.wikipedia.org/wiki/Extreme_learning_machine](https://en.wikipedia.org/wiki/Extreme_learning_machine)

**theory**

[https://community.mycroft.ai/t/why-were-moving-to-deepspeech-on-march-31-privacy-speech-to-text-balance/2729](https://community.mycroft.ai/t/why-were-moving-to-deepspeech-on-march-31-privacy-speech-to-text-balance/2729)

[https://www.kaggle.com/c/tensorflow-speech-recognition-challenge/kernels?sortBy=hotness&group=everyone&pageSize=20&competitionId=7634](https://www.kaggle.com/c/tensorflow-speech-recognition-challenge/kernels?sortBy=hotness&group=everyone&pageSize=20&competitionId=7634)

[https://www.kaggle.com/davids1992/speech-representation-and-data-exploration](https://www.kaggle.com/davids1992/speech-representation-and-data-exploration)

[https://hackaday.com/tag/speech-recognition/](https://hackaday.com/tag/speech-recognition/)

[https://www.liip.ch/en/blog/speech-recognition-with-wit-ai](https://www.liip.ch/en/blog/speech-recognition-with-wit-ai)

[https://fr.wikipedia.org/wiki/Spectrogramme](https://fr.wikipedia.org/wiki/Spectrogramme)

[https://github.com/EMDL/awesome-emdl](https://github.com/EMDL/awesome-emdl)

![](Untitled-f39c2935-cefa-4c63-b396-a30626b1daeb.png)

[https://www.isca-speech.org/iscaweb/index.php/online-archive](https://www.isca-speech.org/iscaweb/index.php/online-archive)

[https://www.isca-speech.org/iscaweb/index.php/sigs](https://www.isca-speech.org/iscaweb/index.php/sigs)

[https://wiki.inria.fr/rosp/Main_Page](https://wiki.inria.fr/rosp/Main_Page)

[https://www.isca-speech.org/iscaweb/index.php/sigs?layout=edit&id=130](https://www.isca-speech.org/iscaweb/index.php/sigs?layout=edit&id=130)

[https://wiki.inria.fr/rosp/Software](https://wiki.inria.fr/rosp/Software)

---

- **ALSA — Linux Audio Card**

    [https://nixingaround.blogspot.com/2016/11/raspberry-pi-zero-usb-audio.html](https://nixingaround.blogspot.com/2016/11/raspberry-pi-zero-usb-audio.html)

    [https://wolfpaulus.com/embedded/raspberrypi2-sr/](https://wolfpaulus.com/embedded/raspberrypi2-sr/)

    [https://github.com/Kitt-AI/snowboy/issues/45](https://github.com/Kitt-AI/snowboy/issues/45)

    [http://docs.kitt.ai/snowboy/#running-on-pi](http://docs.kitt.ai/snowboy/#running-on-pi)

    [http://docs.kitt.ai/snowboy/#access-microphone](http://docs.kitt.ai/snowboy/#access-microphone)

---

[https://github.com/voice-engine/make-a-smart-speaker](https://github.com/voice-engine/make-a-smart-speaker)

[https://hackaday.com/2018/06/21/make-a-natural-language-phone-bot-like-googles-duplex-ai/](https://hackaday.com/2018/06/21/make-a-natural-language-phone-bot-like-googles-duplex-ai/)

[https://github.com/syntithenai/hermod](https://github.com/syntithenai/hermod)

[https://github.com/gooofy/zamia-speech](https://github.com/gooofy/zamia-speech)

[https://github.com/Picovoice](https://github.com/Picovoice)

[https://github.com/cmusphinx/pocketsphinx](https://github.com/cmusphinx/pocketsphinx)

[https://ai.googleblog.com/2019/03/an-all-neural-on-device-speech.html](https://ai.googleblog.com/2019/03/an-all-neural-on-device-speech.html)

[https://github.com/syhw/wer_are_we](https://github.com/syhw/wer_are_we)

- **Streaming End-to-end Speech Recognition For Mobile Devices**

    [https://arxiv.org/abs/1811.06621](https://arxiv.org/abs/1811.06621)

    [https://deepmind.com/research/publications/towards-end-end-speech-recognition-recurrent-neural-networks/](https://deepmind.com/research/publications/towards-end-end-speech-recognition-recurrent-neural-networks/)

    [http://proceedings.mlr.press/v48/amodei16.pdf](http://proceedings.mlr.press/v48/amodei16.pdf)

    [http://iscslp2018.org/images/T4_Towards end-to-end speech recognition.pdf](http://iscslp2018.org/images/T4_Towards%20end-to-end%20speech%20recognition.pdf)

    [https://ai.google/research/pubs/pub46762](https://ai.google/research/pubs/pub46762)

    [https://cs224d.stanford.edu/reports/SongWilliam.pdf](https://cs224d.stanford.edu/reports/SongWilliam.pdf)

    [https://ai.googleblog.com/2017/12/improving-end-to-end-models-for-speech.html](https://ai.googleblog.com/2017/12/improving-end-to-end-models-for-speech.html)

    [https://www.merl.com/publications/docs/TR2018-001.pdf](https://www.merl.com/publications/docs/TR2018-001.pdf)

    [https://ieeexplore.ieee.org/document/7953164](https://ieeexplore.ieee.org/document/7953164)

- **VAD** — **voice activity detection**

    **human voice patterns center around 4000 to 6000Hz. Minding the Nyquist-frequency yields sample rates between 8000 and 12000Hz for best results.**

    **libs**

    [https://www.npmjs.com/package/voice-activity-detection](https://www.npmjs.com/package/voice-activity-detection)

    [https://github.com/wiseman/py-webrtcvad](https://github.com/wiseman/py-webrtcvad) [https://github.com/dpirch/libfvad](https://github.com/dpirch/libfvad)

    [https://chromium.googlesource.com/external/webrtc/+/refs/heads/master/common_audio/vad/](https://chromium.googlesource.com/external/webrtc/+/refs/heads/master/common_audio/vad/)

    [https://github.com/voixen/voixen-vad](https://github.com/voixen/voixen-vad)

    [https://github.com/Snirpo/node-vad](https://github.com/Snirpo/node-vad)

    [https://github.com/kdavis-mozilla/vad.js/blob/master/lib/vad.js](https://github.com/kdavis-mozilla/vad.js/blob/master/lib/vad.js)

    **research**

    [https://ieeexplore.ieee.org/document/8310326](https://ieeexplore.ieee.org/document/8310326)

    [https://www.quora.com/What-are-current-state-of-the-art-algorithms-for-voice-activity-detection](https://www.quora.com/What-are-current-state-of-the-art-algorithms-for-voice-activity-detection)

    [https://www.eurasip.org/Proceedings/Eusipco/Eusipco2009/contents/papers/1569192958.pdf](https://www.eurasip.org/Proceedings/Eusipco/Eusipco2009/contents/papers/1569192958.pdf)

    [https://pdfs.semanticscholar.org/c488/a7ffd940b6d785e75f905c81d99faf4405f2.pdf](https://pdfs.semanticscholar.org/c488/a7ffd940b6d785e75f905c81d99faf4405f2.pdf)

    [https://www.vocal.com/speech-recognition/vad-measuring-detector-performance/](https://www.vocal.com/speech-recognition/vad-measuring-detector-performance/)

    [https://medium.com/linagoralabs/voice-activity-detection-for-voice-user-interface-2d4bb5600ee3](https://medium.com/linagoralabs/voice-activity-detection-for-voice-user-interface-2d4bb5600ee3)

    [https://www.ncbi.nlm.nih.gov/pubmed/25227039](https://www.ncbi.nlm.nih.gov/pubmed/25227039)

    [https://www.hindawi.com/journals/tswj/2014/146040/](https://www.hindawi.com/journals/tswj/2014/146040/)

    [https://asp-eurasipjournals.springeropen.com/articles/10.1186/s13634-015-0277-z](https://asp-eurasipjournals.springeropen.com/articles/10.1186/s13634-015-0277-z)

- **Speaker Diarisation**

    [https://en.wikipedia.org/wiki/Speaker_diarisation](https://en.wikipedia.org/wiki/Speaker_diarisation)

    [https://github.com/Jamiroquai88/VBDiarization](https://github.com/Jamiroquai88/VBDiarization)

- **Speaker Recognition**

    [https://en.wikipedia.org/wiki/Speaker_recognition](https://en.wikipedia.org/wiki/Speaker_recognition)

- **Wakeword/Hotword Detection**

    [http://docs.kitt.ai/snowboy/](http://docs.kitt.ai/snowboy/)

    [https://github.com/picovoice/porcupine](https://github.com/picovoice/porcupine)

    [https://github.com/ARM-software/ML-KWS-for-MCU](https://github.com/ARM-software/ML-KWS-for-MCU)

- **ASR — Mozilla DeepSpeech**

    [https://github.com/mozilla/DeepSpeech/wiki/Meeting-Notes](https://github.com/mozilla/DeepSpeech/wiki/Meeting-Notes)

    [https://discourse.mozilla.org/c/deep-speech](https://discourse.mozilla.org/c/deep-speech)

    [https://github.com/mozilla/DeepSpeech/releases](https://github.com/mozilla/DeepSpeech/releases)

    [https://github.com/mozilla/DeepSpeech/issues](https://github.com/mozilla/DeepSpeech/issues)

    [https://github.com/mozilla/DeepSpeech/wiki#frequently-asked-questions](https://github.com/mozilla/DeepSpeech/wiki#frequently-asked-questions)

- **ASR — Kaldi**

    [http://kaldi-asr.org/doc/kaldi_for_dummies.html#kaldi_for_dummies_directories](http://kaldi-asr.org/doc/kaldi_for_dummies.html#kaldi_for_dummies_directories)

    [https://www.eleanorchodroff.com/tutorial/kaldi/training-overview.html](https://www.eleanorchodroff.com/tutorial/kaldi/training-overview.html)

    [https://medium.com/descript/the-state-of-automatic-speech-recognition-q-a-with-kaldis-dan-povey-c860aada9b85](https://medium.com/descript/the-state-of-automatic-speech-recognition-q-a-with-kaldis-dan-povey-c860aada9b85)

    [https://github.com/YoavRamon/awesome-kaldi](https://github.com/YoavRamon/awesome-kaldi)

    [https://github.com/JRMeyer/multi-task-kaldi/](https://github.com/JRMeyer/multi-task-kaldi/)

    [http://www.openfst.org/twiki/bin/view/FST/WebHome](http://www.openfst.org/twiki/bin/view/FST/WebHome)

    [http://jrmeyer.github.io/](http://jrmeyer.github.io/)

    [http://www.danielpovey.com/kaldi-lectures.html](http://www.danielpovey.com/kaldi-lectures.html)

    [https://engineering.jhu.edu/clsp/wp-content/uploads/sites/75/2016/06/Building-Speech-Recognition-Systems-with-the-Kaldi-Toolkit.pdf](https://engineering.jhu.edu/clsp/wp-content/uploads/sites/75/2016/06/Building-Speech-Recognition-Systems-with-the-Kaldi-Toolkit.pdf)

    [https://github.com/YoavRamon/awesome-kaldi](https://github.com/YoavRamon/awesome-kaldi)

    - **theory**

        [https://towardsdatascience.com/how-to-start-with-kaldi-and-speech-recognition-a9b7670ffff6](https://towardsdatascience.com/how-to-start-with-kaldi-and-speech-recognition-a9b7670ffff6)

        [http://jrmeyer.github.io/asr/2016/02/01/Kaldi-notes.html](http://jrmeyer.github.io/asr/2016/02/01/Kaldi-notes.html)

    **repo**: [https://github.com/kaldi-asr/kaldi/](https://github.com/kaldi-asr/kaldi/)

    **doc**: [http://kaldi-asr.org/doc/](http://kaldi-asr.org/doc/)

    **help**: [https://groups.google.com/forum/#!forum/kaldi-help](https://groups.google.com/forum/#!forum/kaldi-help)

    **aws**: [http://jrmeyer.github.io/asr/2017/10/13/Kaldi-AWS.html](http://jrmeyer.github.io/asr/2017/10/13/Kaldi-AWS.html)

    **models**: [http://kaldi-asr.org/models.html](http://kaldi-asr.org/models.html)

    [https://github.com/alumae/kaldi-offline-transcriber](https://github.com/alumae/kaldi-offline-transcriber)

    [http://jrmeyer.github.io/asr/2016/12/15/DNN-AM-Kaldi.html](http://jrmeyer.github.io/asr/2016/12/15/DNN-AM-Kaldi.html)

    [http://jrmeyer.github.io/asr/2016/01/26/Installing-Kaldi.html](http://jrmeyer.github.io/asr/2016/01/26/Installing-Kaldi.html)

    [https://www.eleanorchodroff.com/tutorial/kaldi/index.html](https://www.eleanorchodroff.com/tutorial/kaldi/index.html)

    [https://medium.com/@nikhilamunipalli/simple-guide-to-kaldi-an-efficient-open-source-speech-recognition-tool-for-extreme-beginners-98a48bb34756](https://medium.com/@nikhilamunipalli/simple-guide-to-kaldi-an-efficient-open-source-speech-recognition-tool-for-extreme-beginners-98a48bb34756)

    [http://www.dsp.agh.edu.pl/_media/pl:dydaktyka:kaldi_for_dummies.pdf](http://www.dsp.agh.edu.pl/_media/pl:dydaktyka:kaldi_for_dummies.pdf)

- **ASR - Hybrid**

    [https://github.com/mravanelli/pytorch-kaldi](https://github.com/mravanelli/pytorch-kaldi)

- **ASR — Baidu DeepSpeech2**

    dp1: [https://arxiv.org/abs/1412.5567](https://arxiv.org/abs/1412.5567)

    dp2: [https://www.semanticscholar.org/paper/Deep-Speech-2-%3A-End-to-End-Speech-Recognition-in-Amodei-Ananthanarayanan/8e0eacf11a22b9705a262e908f17b1704fd21fa7](https://www.semanticscholar.org/paper/Deep-Speech-2-%3A-End-to-End-Speech-Recognition-in-Amodei-Ananthanarayanan/8e0eacf11a22b9705a262e908f17b1704fd21fa7)

    [https://github.com/PaddlePaddle/DeepSpeech](https://github.com/PaddlePaddle/DeepSpeech)

    [https://hn.svelte.technology/item/15837365](https://hn.svelte.technology/item/15837365)

- **ASR — FB Wav2Letter**

    **flashlight**: [https://github.com/facebookresearch/flashlight](https://github.com/facebookresearch/flashlight)

    [https://code.fb.com/ai-research/wav2letter/](https://code.fb.com/ai-research/wav2letter/)

    [https://github.com/facebookresearch/wav2letter](https://github.com/facebookresearch/wav2letter)

- **CTC Decoder**

    [https://github.com/lingochamp/kaldi-ctc](https://github.com/lingochamp/kaldi-ctc)

    [https://towardsdatascience.com/intuitively-understanding-connectionist-temporal-classification-3797e43a86c](https://towardsdatascience.com/intuitively-understanding-connectionist-temporal-classification-3797e43a86c)

    [https://towardsdatascience.com/beam-search-decoding-in-ctc-trained-neural-networks-5a889a3d85a7](https://towardsdatascience.com/beam-search-decoding-in-ctc-trained-neural-networks-5a889a3d85a7)

    [https://towardsdatascience.com/word-beam-search-a-ctc-decoding-algorithm-b051d28f3d2e](https://towardsdatascience.com/word-beam-search-a-ctc-decoding-algorithm-b051d28f3d2e)

- **NLU — natural language understanding**

    [https://snips-nlu.readthedocs.io/en/latest/](https://snips-nlu.readthedocs.io/en/latest/)

    [https://github.com/snipsco/snips-nlu-rs](https://github.com/snipsco/snips-nlu-rs)

    [https://course.spacy.io/](https://course.spacy.io/)

- **Dialog**

    [https://dialogflow.com/](https://dialogflow.com/)

---

## Plus

- **Vocal Synthesis**

    [https://google.github.io/tacotron/publications/tacotron2/index.html](https://google.github.io/tacotron/publications/tacotron2/index.html)

    [https://google.github.io/tacotron/](https://google.github.io/tacotron/)

    [http://www.x90x90x90.com/deeplearning-mise-en-oeuvre-du-modele-tacotron-pour-la-synthese-de-la-parole-en-francais/](http://www.x90x90x90.com/deeplearning-mise-en-oeuvre-du-modele-tacotron-pour-la-synthese-de-la-parole-en-francais/)

- **LipSync**

    [http://grail.cs.washington.edu/projects/AudioToObama/](http://grail.cs.washington.edu/projects/AudioToObama/)

- **Sentence boundary detection**

    [https://github.com/mozilla/DeepSpeech/issues/1905](https://github.com/mozilla/DeepSpeech/issues/1905)

- **Segmentation**

    [https://github.com/bedapudi6788/deepsegment](https://github.com/bedapudi6788/deepsegment)

- **Punctuator**

    [https://github.com/ottokart/punctuator2](https://github.com/ottokart/punctuator2)

- **forced aligner**

    [https://lowerquality.com/gentle/](https://lowerquality.com/gentle/)

---

## **Training (magic number: 100k - 10k - 3k)**

End-to-end STT only generalizes/works well if you have a lot of data, the magic number is around 10000 hours. Some companies like Google, Baidu and Microsoft apply end-to-end and people like Andrew Ng (worked at Baidu’s DeepSpeech Project) reinforce this need for a large corpus.

The approach for small companies is an hybrid solution using HMM/DNN, usually using some toolkit like Kaldi. This approach involves preprocessing the data, working with phonemes and some alignment before feeding it all to training.

[https://github.com/tensorflow/tensorflow/tree/master/tensorflow/lite/experimental/micro/examples/micro_speech#creating-your-own-model](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/lite/experimental/micro/examples/micro_speech#creating-your-own-model)

[https://medium.com/tensorflow/introducing-tensorflow-datasets-c7f01f7e19f3](https://medium.com/tensorflow/introducing-tensorflow-datasets-c7f01f7e19f3)

- **Corpora Tools**

    [https://github.com/mozilla/voice-corpus-tool](https://github.com/mozilla/voice-corpus-tool)

    [https://stackoverflow.com/questions/20014064/how-to-batch-split-audio-files-wherever-there-is-silence](https://stackoverflow.com/questions/20014064/how-to-batch-split-audio-files-wherever-there-is-silence)

- **Voice Corpus**
    - french model

        [https://discourse.mozilla.org/t/tutorial-how-i-trained-a-specific-french-model-to-control-my-robot/22830](https://discourse.mozilla.org/t/tutorial-how-i-trained-a-specific-french-model-to-control-my-robot/22830)

        [https://github.com/mozilla/DeepSpeech/issues/1576](https://github.com/mozilla/DeepSpeech/issues/1576)

        [http://tommygingras.com/tensorflow-deepspeech-francais/](http://tommygingras.com/tensorflow-deepspeech-francais/)

        [https://discourse.mozilla.org/t/interesting-blog-post-on-the-importance-of-training-data-quality/28966](https://discourse.mozilla.org/t/interesting-blog-post-on-the-importance-of-training-data-quality/28966)

    [https://voxforge.org/](https://voxforge.org/)

    **librispeech**: [http://www.openslr.org/12/](http://www.openslr.org/12/)

    **common voice**: [https://voice.mozilla.org/en/datasets](https://voice.mozilla.org/en/datasets)

    [https://discourse.mozilla.org/t/longer-audio-files-with-deep-speech/22784/7](https://discourse.mozilla.org/t/longer-audio-files-with-deep-speech/22784/7?u=jehoshua)

    [https://voices18.github.io/](https://voices18.github.io/)

    [https://freesound.org/](https://freesound.org/)

    [https://www.otr.net/](https://www.otr.net/)

    [https://forvo.com/](https://forvo.com/)

    [https://gitlab.com/nicolaspanel/TrainingSpeech](https://gitlab.com/nicolaspanel/TrainingSpeech)

- **Docker**

    [https://github.com/MainRo/docker-deepspeech-server](https://github.com/MainRo/docker-deepspeech-server)

- **GPU / CUDA**

    [https://github.com/NVIDIA/nvidia-docker](https://github.com/NVIDIA/nvidia-docker)

    [https://powersj.io/post/ubuntu-server-nvidia-cuda/](https://powersj.io/post/ubuntu-server-nvidia-cuda/)

- **TensorFlow 2.0**

    [https://www.tensorflow.org/alpha](https://www.tensorflow.org/alpha)

    [https://github.com/tensorflow/docs/blob/master/site/en/r2/guide/effective_tf2.md](https://github.com/tensorflow/docs/blob/master/site/en/r2/guide/effective_tf2.md)

    [https://github.com/mozilla/tensorflow](https://github.com/mozilla/tensorflow)

[https://medium.com/tensorflow/mit-deep-learning-basics-introduction-and-overview-with-tensorflow-355bcd26baf0](https://medium.com/tensorflow/mit-deep-learning-basics-introduction-and-overview-with-tensorflow-355bcd26baf0)

---

## Mobile Model Interpreter & Inference & Hardware Acceleration

[https://medium.com/tensorflow/build-ai-that-works-offline-with-coral-dev-board-edge-tpu-and-tensorflow-lite-7074a9fd0172](https://medium.com/tensorflow/build-ai-that-works-offline-with-coral-dev-board-edge-tpu-and-tensorflow-lite-7074a9fd0172)

[https://medium.com/tensorflow/introducing-tensorflow-federated-a4147aa20041](https://medium.com/tensorflow/introducing-tensorflow-federated-a4147aa20041)

[https://github.com/snipsco/tract](https://github.com/snipsco/tract)

[https://www.slideshare.net/a/squeezing-deep-learning-into-mobile-phones](https://www.slideshare.net/a/squeezing-deep-learning-into-mobile-phones)

[https://maker.pro/raspberry-pi/tutorial/the-best-voice-recognition-software-for-raspberry-pi](https://maker.pro/raspberry-pi/tutorial/the-best-voice-recognition-software-for-raspberry-pi)

[https://onnx.ai/](https://onnx.ai/)

- **Hardware Accel**

    [https://trac.ffmpeg.org/wiki/HWAccelIntro](https://trac.ffmpeg.org/wiki/HWAccelIntro)

- **TensorFlow Lite**

    [https://www.tensorflow.org/api_docs/python/tf/lite/TFLiteConverter](https://www.tensorflow.org/api_docs/python/tf/lite/TFLiteConverter)

    [https://www.tensorflow.org/lite/performance/gpu](https://www.tensorflow.org/lite/performance/gpu)

    [https://www.tensorflow.org/lite/guide/build_rpi](https://www.tensorflow.org/lite/guide/build_rpi) -- tensorflow/lite/tools/make/gen/rpi_armv7l/lib/libtensorflow-lite.a

    [https://github.com/tensorflow/tensorflow/tree/master/tensorflow/lite/tools/pip_package](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/lite/tools/pip_package)

    [https://google.github.io/flatbuffers/](https://google.github.io/flatbuffers/)

    [https://github.com/mozilla/DeepSpeech/issues/1224](https://github.com/mozilla/DeepSpeech/issues/1224)

    [https://www.tensorflow.org/lite/guide/hosted_models](https://www.tensorflow.org/lite/guide/hosted_models)

    [https://coral.withgoogle.com/products/accelerator/](https://coral.withgoogle.com/products/accelerator/)

    [https://github.com/mozilla/DeepSpeech/issues/1346](https://github.com/mozilla/DeepSpeech/issues/1346)

    [https://medium.com/snips-ai/how-we-made-tensorflow-run-on-a-raspberry-pi-using-rust-7478f7a31329](https://medium.com/snips-ai/how-we-made-tensorflow-run-on-a-raspberry-pi-using-rust-7478f7a31329)

    [https://github.com/snipsco/tensorflow-build](https://github.com/snipsco/tensorflow-build)

    [https://github.com/tensorflow/tensorflow/tree/master/tensorflow/lite/experimental/micro](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/lite/experimental/micro)

    [https://codelabs.developers.google.com/codelabs/sparkfun-tensorflow/](https://codelabs.developers.google.com/codelabs/sparkfun-tensorflow/)

- **Neural Hardware**

    [https://www.sparkfun.com/products/15170](https://www.sparkfun.com/products/15170)

---

## Infrastructure

- **Websocket**

    [https://github.com/daanzu/deepspeech-websocket-server](https://github.com/daanzu/deepspeech-websocket-server)

- **Gstreamer**

    [https://github.com/alumae?tab=repositories](https://github.com/alumae?tab=repositories)

    [https://github.com/alumae/kaldi-gstreamer-server](https://github.com/alumae/kaldi-gstreamer-server)

    [https://github.com/jcsilva/docker-kaldi-gstreamer-server](https://github.com/jcsilva/docker-kaldi-gstreamer-server)

    [http://blog.mikeasoft.com/2017/12/30/speech-recognition-mozillas-deepspeech-gstreamer-and-ibus/#comment-1120291](http://blog.mikeasoft.com/2017/12/30/speech-recognition-mozillas-deepspeech-gstreamer-and-ibus/#comment-1120291)

    [https://github.com/Elleo/gst-deepspeech](https://github.com/Elleo/gst-deepspeech)

    [http://blog.mikeasoft.com/2017/12/30/speech-recognition-mozillas-deepspeech-gstreamer-and-ibus/](http://blog.mikeasoft.com/2017/12/30/speech-recognition-mozillas-deepspeech-gstreamer-and-ibus/)

    [https://gitlab.freedesktop.org/gstreamer](https://gitlab.freedesktop.org/gstreamer)
    pip3 install deepspeech

        apt-get install libgstreamer1.0-0 gstreamer1.0-plugins-base gstreamer1.0-plugins-good gstreamer1.0-plugins-bad gstreamer1.0-plugins-ugly gstreamer1.0-libav gstreamer1.0-doc gstreamer1.0-tools gstreamer1.0-x gstreamer1.0-alsa gstreamer1.0-gl gstreamer1.0-gtk3 gstreamer1.0-qt5 gstreamer1.0-pulseaudio

- **Ibus**

    [https://github.com/Elleo/ibus-deepspeech](https://github.com/Elleo/ibus-deepspeech)